{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMTesting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPi5X5dyzMXm6TZNAPZ9o6Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamAshbee/minigan-catalyst/blob/master/GANLSTMTesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8AVgTkbwGD6"
      },
      "source": [
        "#https://github.com/bhpfelix/PyTorch-Time-Series-Classification-Benchmarks/blob/master/MaterialRecognitionModels.ipynb\r\n",
        "import shutil, os, csv, itertools, glob\r\n",
        "\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.autograd import Variable\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import pickle as pk\r\n",
        "\r\n",
        "cuda = torch.cuda.is_available()\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRq9xB1nwKgp"
      },
      "source": [
        "## Multilayer LSTM based classifier taking in 200 dimensional fixed time series inputs\r\n",
        "class LSTMClassifier(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, in_dim, hidden_dim, num_layers, dropout, bidirectional, num_classes, batch_size):\r\n",
        "        super(LSTMClassifier, self).__init__()\r\n",
        "        self.arch = 'lstm'\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.num_dir = 2 if bidirectional else 1\r\n",
        "        self.num_layers = num_layers\r\n",
        "\r\n",
        "        self.lstm = nn.LSTM(\r\n",
        "                input_size=in_dim,\r\n",
        "                hidden_size=hidden_dim,\r\n",
        "                num_layers=num_layers,\r\n",
        "                dropout=dropout,\r\n",
        "                bidirectional=bidirectional\r\n",
        "            )\r\n",
        "\r\n",
        "        self.hidden2label = nn.Sequential(\r\n",
        "            nn.Linear(hidden_dim*self.num_dir, hidden_dim),\r\n",
        "            nn.ReLU(True),\r\n",
        "            nn.Dropout(),\r\n",
        "            nn.Linear(hidden_dim, hidden_dim),\r\n",
        "            nn.ReLU(True),\r\n",
        "            nn.Dropout(),\r\n",
        "            nn.Linear(hidden_dim, num_classes),\r\n",
        "            nn.Sigmoid()\r\n",
        "        )\r\n",
        "\r\n",
        "        self.hidden = self.init_hidden()\r\n",
        "\r\n",
        "    def init_hidden(self):\r\n",
        "        if cuda:\r\n",
        "            h0 = Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim).cuda())\r\n",
        "            c0 = Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim).cuda())\r\n",
        "        else:\r\n",
        "            h0 = Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim))\r\n",
        "            c0 = Variable(torch.zeros(self.num_layers*self.num_dir, self.batch_size, self.hidden_dim))\r\n",
        "        return (h0, c0)\r\n",
        "\r\n",
        "    def forward(self, x): # x is (batch_size, 1, 200), permute to (200, batch_size, 1)\r\n",
        "        x = x.permute(2, 0, 1)\r\n",
        "        # See: https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/2\r\n",
        "        lstm_out, (h, c) = self.lstm(x, self.init_hidden())\r\n",
        "        y  = self.hidden2label(lstm_out[-1])\r\n",
        "        return y\r\n",
        "    \r\n",
        "    def setBatchSize(self, batch_size = 1):\r\n",
        "        self.batch_size = batch_size\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqG7ptZwwK_5"
      },
      "source": [
        "def get_models(): # tuples of (batch_size, model)\r\n",
        "    return [\r\n",
        "         LSTMClassifier(\r\n",
        "            in_dim=2,\r\n",
        "            hidden_dim=120,\r\n",
        "            num_layers=3,\r\n",
        "            dropout=0.8,\r\n",
        "            bidirectional=True,\r\n",
        "            num_classes=1,#bce loss for discriminator\r\n",
        "            batch_size=256\r\n",
        "        )\r\n",
        "    ]\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6ZxtcLqxZgY"
      },
      "source": [
        "model = get_models()[0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF7gAqCvxhTZ",
        "outputId": "3a3a9481-d759-4244-f578-d487910d07bf"
      },
      "source": [
        "model"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMClassifier(\n",
              "  (lstm): LSTM(2, 120, num_layers=3, dropout=0.8, bidirectional=True)\n",
              "  (hidden2label): Sequential(\n",
              "    (0): Linear(in_features=240, out_features=120, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=120, out_features=120, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=120, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svNe79rgxkM4"
      },
      "source": [
        "x = torch.ones(256,2,200).float().cuda()\r\n",
        "model = model.cuda()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78DaiuJHx0zH",
        "outputId": "4eea9aa9-dcc8-409e-cf8b-4e28da607bd2"
      },
      "source": [
        "model(x).shape\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ9VNpOAyEDa"
      },
      "source": [
        "# Dummy Dataset for testing purpose only\r\n",
        "class DummyDataset(Dataset):\r\n",
        "    \"\"\"Time Series dataset.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, numclasses=15):\r\n",
        "        self.numclasses = numclasses\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return 512\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        data = None\r\n",
        "        y = None\r\n",
        "        if torch.rand(1)[0] > .5:\r\n",
        "          data = torch.ones(2,200).float()\r\n",
        "          y = float(1)\r\n",
        "        else:\r\n",
        "          data = torch.zeros(2,200).float()\r\n",
        "          y = float(0)\r\n",
        "          \r\n",
        "        return data, y\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0hJn9D62lGQ"
      },
      "source": [
        "data = DummyDataset(numclasses=2)\r\n",
        "D = model.cuda()\r\n",
        "train_loader = torch.utils.data.DataLoader(\r\n",
        "    data,\r\n",
        "    batch_size=256, shuffle=True)\r\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=.0001, betas=(0.5, 0.999))\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g1o8TiU41wT"
      },
      "source": [
        "bce = torch.nn.BCELoss()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeiGwmVrRyX5",
        "outputId": "e0fa1e02-0741-41cf-9748-77bec1f0c3c2"
      },
      "source": [
        "for epoch in range(1):\r\n",
        "  for x_, y_ in train_loader:\r\n",
        "    y_ = y_.unsqueeze(dim = 1).float().cuda()\r\n",
        "    x_ = x_.cuda()\r\n",
        "\r\n",
        "    D_result = D(x_)\r\n",
        "    #print(D_result)\r\n",
        "    #break\r\n",
        "    #print(D_result)\r\n",
        "    D_train_loss = bce(D_result, y_)\r\n",
        "    print('D_train_loss', D_train_loss)\r\n",
        "    D_train_loss.backward()\r\n",
        "    D_optimizer.step()\r\n",
        "\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D_train_loss tensor(0.6944, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "D_train_loss tensor(0.6936, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnbrMEJQYxGp"
      },
      "source": [
        "data1 = torch.ones(1,2,200).float()\r\n",
        "y1 = torch.ones(1,1)\r\n",
        "data0 = torch.zeros(1,2,200).float()\r\n",
        "y0 = torch.zeros(1,1)\r\n",
        "        "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyCyjrZi-mHH",
        "outputId": "a4e16b1f-ded6-4451-d7bd-d8aebfb21136"
      },
      "source": [
        "for x, y in train_loader:\r\n",
        "  x = x.cuda()\r\n",
        "  print(torch.sum(model(x)>.5))\r\n",
        "  print(torch.sum(y))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(65, device='cuda:0')\n",
            "tensor(126., dtype=torch.float64)\n",
            "tensor(62, device='cuda:0')\n",
            "tensor(139., dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB_GMORh-3v1"
      },
      "source": [
        "# Dummy Dataset for testing purpose only\r\n",
        "class DummyDataset2(Dataset):\r\n",
        "    \"\"\"Time Series dataset.\"\"\"\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return 512\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        data = None\r\n",
        "        y = None\r\n",
        "        ind = [x for x in range(200)]\r\n",
        "        ind = torch.tensor(ind).float()\r\n",
        "        if torch.rand(1)[0] > .5:\r\n",
        "          data = torch.ones(2,200).float()\r\n",
        "          data[0,:] = torch.sin(ind) +torch.rand(1)[0]\r\n",
        "          data[1,:] = torch.cos(ind) +torch.rand(1)[0]\r\n",
        "          y = float(1)\r\n",
        "        else:\r\n",
        "          data = torch.ones(2,200).float()\r\n",
        "          \r\n",
        "          data[0,:] = torch.sin(ind*2.0) +torch.rand(1)[0] \r\n",
        "          data[1,:] = torch.cos(ind*2.0) +torch.rand(1)[0] \r\n",
        "          y = float(0)\r\n",
        "          \r\n",
        "        return data, y\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnUBySNNd_-9"
      },
      "source": [
        "data = DummyDataset2()\r\n",
        "D = get_models()[0].cuda()\r\n",
        "train_loader = torch.utils.data.DataLoader(\r\n",
        "    data,\r\n",
        "    batch_size=256, shuffle=True)\r\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=.0001, betas=(0.5, 0.999))\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yUYcwNpe8yX",
        "outputId": "87694cb2-1a2d-42db-f8e6-a6e1eac4753b"
      },
      "source": [
        "for epoch in range(1):\r\n",
        "  for x_, y_ in train_loader:\r\n",
        "    y_ = y_.unsqueeze(dim = 1).float().cuda()\r\n",
        "    x_ = x_.cuda()\r\n",
        "\r\n",
        "    D_result = D(x_)\r\n",
        "    #print(D_result)\r\n",
        "    #break\r\n",
        "    #print(D_result)\r\n",
        "    D_train_loss = bce(D_result, y_)\r\n",
        "    print('D_train_loss', D_train_loss)\r\n",
        "    D_train_loss.backward()\r\n",
        "    D_optimizer.step()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D_train_loss tensor(0.6932, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
            "D_train_loss tensor(0.6903, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WHT5frSfmqz"
      },
      "source": [
        "#https://github.com/bhpfelix/PyTorch-Time-Series-Classification-Benchmarks/blob/master/MaterialRecognitionModels.ipynb"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f5KB0Duh9W5"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.hub import load_state_dict_from_url\r\n",
        "\r\n",
        "\r\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\r\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\r\n",
        "           'wide_resnet50_2', 'wide_resnet101_2']\r\n",
        "\r\n",
        "\r\n",
        "model_urls = {\r\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\r\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\r\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\r\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\r\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\r\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\r\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\r\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\r\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\r\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\r\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\r\n",
        "\r\n",
        "\r\n",
        "def conv1x1(in_planes, out_planes, stride=1):\r\n",
        "    \"\"\"1x1 convolution\"\"\"\r\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\r\n",
        "\r\n",
        "\r\n",
        "class BasicBlock(nn.Module):\r\n",
        "    expansion = 1\r\n",
        "\r\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\r\n",
        "                 base_width=64, dilation=1, norm_layer=None):\r\n",
        "        super(BasicBlock, self).__init__()\r\n",
        "        if norm_layer is None:\r\n",
        "            norm_layer = nn.BatchNorm2d\r\n",
        "        if groups != 1 or base_width != 64:\r\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\r\n",
        "        if dilation > 1:\r\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\r\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\r\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\r\n",
        "        self.bn1 = norm_layer(planes)\r\n",
        "        self.relu = nn.ReLU(inplace=False)\r\n",
        "        self.conv2 = conv3x3(planes, planes)\r\n",
        "        self.bn2 = norm_layer(planes)\r\n",
        "        self.downsample = downsample\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        identity = x\r\n",
        "\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            identity = self.downsample(x)\r\n",
        "\r\n",
        "        out += identity\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class Bottleneck(nn.Module):\r\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\r\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\r\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\r\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\r\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\r\n",
        "\r\n",
        "    expansion = 4\r\n",
        "\r\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\r\n",
        "                 base_width=64, dilation=1, norm_layer=None):\r\n",
        "        super(Bottleneck, self).__init__()\r\n",
        "        if norm_layer is None:\r\n",
        "            norm_layer = nn.BatchNorm2d\r\n",
        "        width = int(planes * (base_width / 64.)) * groups\r\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\r\n",
        "        self.conv1 = conv1x1(inplanes, width)\r\n",
        "        self.bn1 = norm_layer(width)\r\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\r\n",
        "        self.bn2 = norm_layer(width)\r\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\r\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\r\n",
        "        self.relu = nn.ReLU(inplace=False)\r\n",
        "        self.downsample = downsample\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        identity = x\r\n",
        "\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        out = self.conv3(out)\r\n",
        "        out = self.bn3(out)\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            identity = self.downsample(x)\r\n",
        "\r\n",
        "        out += identity\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class ResNet(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, block, layers, num_classes=2000, zero_init_residual=False,\r\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\r\n",
        "                 norm_layer=None):\r\n",
        "        super(ResNet, self).__init__()\r\n",
        "        if norm_layer is None:\r\n",
        "            norm_layer = nn.BatchNorm2d\r\n",
        "        self._norm_layer = norm_layer\r\n",
        "\r\n",
        "        self.inplanes = 64\r\n",
        "        self.dilation = 1\r\n",
        "        if replace_stride_with_dilation is None:\r\n",
        "            # each element in the tuple indicates if we should replace\r\n",
        "            # the 2x2 stride with a dilated convolution instead\r\n",
        "            replace_stride_with_dilation = [False, False, False]\r\n",
        "        if len(replace_stride_with_dilation) != 3:\r\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\r\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\r\n",
        "        self.groups = groups\r\n",
        "        self.base_width = width_per_group\r\n",
        "        self.conv1 = nn.Conv2d(5, self.inplanes, kernel_size=7, stride=2, padding=3,\r\n",
        "                               bias=False)\r\n",
        "        self.bn1 = norm_layer(self.inplanes)\r\n",
        "        self.relu = nn.ReLU(inplace=False)\r\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\r\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\r\n",
        "                                       dilate=replace_stride_with_dilation[0])\r\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\r\n",
        "                                       dilate=replace_stride_with_dilation[1])\r\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\r\n",
        "                                       dilate=replace_stride_with_dilation[2])\r\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\r\n",
        "\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\r\n",
        "                nn.init.constant_(m.weight, 1)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "\r\n",
        "        # Zero-initialize the last BN in each residual branch,\r\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\r\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\r\n",
        "        if zero_init_residual:\r\n",
        "            for m in self.modules():\r\n",
        "                if isinstance(m, Bottleneck):\r\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\r\n",
        "                elif isinstance(m, BasicBlock):\r\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\r\n",
        "\r\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\r\n",
        "        norm_layer = self._norm_layer\r\n",
        "        downsample = None\r\n",
        "        previous_dilation = self.dilation\r\n",
        "        if dilate:\r\n",
        "            self.dilation *= stride\r\n",
        "            stride = 1\r\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\r\n",
        "            downsample = nn.Sequential(\r\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\r\n",
        "                norm_layer(planes * block.expansion),\r\n",
        "            )\r\n",
        "\r\n",
        "        layers = []\r\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\r\n",
        "                            self.base_width, previous_dilation, norm_layer))\r\n",
        "        self.inplanes = planes * block.expansion\r\n",
        "        for _ in range(1, blocks):\r\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\r\n",
        "                                base_width=self.base_width, dilation=self.dilation,\r\n",
        "                                norm_layer=norm_layer))\r\n",
        "\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "\r\n",
        "    def _forward_impl(self, x):\r\n",
        "        # See note [TorchScript super()]\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        x = self.maxpool(x)\r\n",
        "\r\n",
        "        x = self.layer1(x)\r\n",
        "        x = self.layer2(x)\r\n",
        "        x = self.layer3(x)\r\n",
        "        x = self.layer4(x)\r\n",
        "\r\n",
        "        x = self.avgpool(x)\r\n",
        "        x = torch.flatten(x, 1)\r\n",
        "        x = self.fc(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self._forward_impl(x)\r\n",
        "\r\n",
        "\r\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\r\n",
        "    model = ResNet(block, layers, **kwargs)\r\n",
        "    if pretrained:\r\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\r\n",
        "                                              progress=progress)\r\n",
        "        model.load_state_dict(state_dict)\r\n",
        "    return model\r\n",
        "\r\n",
        "def resnet18(pretrained=False, progress=True, **kwargs):\r\n",
        "    \"\"\"ResNet-18 model from\r\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\r\n",
        "\r\n",
        "    Args:\r\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "    \"\"\"\r\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\r\n",
        "                   **kwargs)\r\n",
        "\r\n",
        "def resnext101_32x8d(pretrained=False, progress=True, **kwargs):\r\n",
        "    \"\"\"ResNeXt-101 32x8d model from\r\n",
        "    Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>\r\n",
        "\r\n",
        "    Args:\r\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "    \"\"\"\r\n",
        "    kwargs['groups'] = 32\r\n",
        "    kwargs['width_per_group'] = 8\r\n",
        "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\r\n",
        "                   pretrained, progress, **kwargs)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9R2SfxuerAr"
      },
      "source": [
        "generator = resnet18(pretrained=False, progress=True).cuda()\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojGerScQgoFz"
      },
      "source": [
        "a = torch.ones(256, 5, 32, 32).float().cuda()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oWFEPk8gvzV",
        "outputId": "40e667aa-6eca-4d8e-9cb0-cf0a20ff09f4"
      },
      "source": [
        "generator(a)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0177, 0.0293, 0.0113,  ..., 0.0056, 0.0023, 0.0123],\n",
              "        [0.0177, 0.0293, 0.0113,  ..., 0.0056, 0.0023, 0.0123],\n",
              "        [0.0177, 0.0293, 0.0113,  ..., 0.0056, 0.0023, 0.0123],\n",
              "        ...,\n",
              "        [0.0177, 0.0293, 0.0113,  ..., 0.0056, 0.0023, 0.0123],\n",
              "        [0.0177, 0.0293, 0.0113,  ..., 0.0056, 0.0023, 0.0123],\n",
              "        [0.0177, 0.0293, 0.0113,  ..., 0.0056, 0.0023, 0.0123]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rHJAOZpgypn"
      },
      "source": [
        "discriminator = get_models()[0].cuda()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcqG4Q1elBNR",
        "outputId": "816af642-b3f6-4db6-a55f-298311731584"
      },
      "source": [
        "discriminator"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMClassifier(\n",
              "  (lstm): LSTM(2, 120, num_layers=3, dropout=0.8, bidirectional=True)\n",
              "  (hidden2label): Sequential(\n",
              "    (0): Linear(in_features=240, out_features=120, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=120, out_features=120, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=120, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L0x9EWIlCc2"
      },
      "source": [
        "#x = torch.ones(256,2,200).float().cuda()\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-od8cM-llw1y"
      },
      "source": [
        "#discriminator(x).shape"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um8vp_k4lzAC"
      },
      "source": [
        "#gt = torch.ones(256,1).cuda()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhjn_DGKl-Tc"
      },
      "source": [
        "#loss = nn.BCELoss()\r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK6zWjWfmugt"
      },
      "source": [
        "#output = discriminator(x)\r\n",
        "#loss(output,gt)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soQ57OC5nBU7"
      },
      "source": [
        "#result = loss(output,gt)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIAxh4FfnWOo"
      },
      "source": [
        "#result.backward()\r\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0gLi2M5nXbE"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqKfoFF-nwne"
      },
      "source": [
        "og = generator(a)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLZ5DfbioCJo",
        "outputId": "ec742a9b-8eba-4dd8-cbe9-34891ee4cd10"
      },
      "source": [
        "og.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 2000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJwuT75woEP-"
      },
      "source": [
        "ogr = og.reshape(-1,2,1000)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l94Ofs1JotLq"
      },
      "source": [
        "od = discriminator(ogr)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGR0Cri1o2pY",
        "outputId": "06f6f2e2-81bc-4acf-d203-a431010d130c"
      },
      "source": [
        "od.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([256, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tyih5KEyo3bU"
      },
      "source": [
        "gtd = torch.ones(256,1).cuda()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNmk5VNGo_FW"
      },
      "source": [
        "loss = nn.BCELoss()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCSuG3qopB4q"
      },
      "source": [
        "lossout = loss(od,gtd)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p_jOpPopHOG"
      },
      "source": [
        "lossout.backward()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBec9YDepIOK"
      },
      "source": [
        "G =  resnet18(pretrained=False, progress=True).cuda()\r\n",
        "D = get_models()[0].cuda() \r\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=.0001, betas=(0.5, 0.999))\r\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=.0001, betas=(0.5, 0.999))\r\n",
        "a = torch.ones(256, 5, 32, 32).float().cuda()\r\n",
        "og = generator(a)\r\n",
        "ogr = og.reshape(-1,2,1000)\r\n",
        "od = discriminator(ogr)\r\n",
        "gtd = torch.ones(256,1).cuda()\r\n",
        "loss = nn.BCELoss()\r\n",
        "lossout = loss(od,gtd)\r\n",
        "lossout.backward()\r\n",
        "G_optimizer.step()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZntMxqhCsLCG"
      },
      "source": [
        "G =  resnet18(pretrained=False, progress=True).cuda()\r\n",
        "D = get_models()[0].cuda() \r\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=.0001, betas=(0.5, 0.999))\r\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=.0001, betas=(0.5, 0.999))\r\n",
        "a = torch.ones(256, 5, 32, 32).float().cuda()\r\n",
        "og = generator(a)\r\n",
        "ogr = og.reshape(-1,2,1000)\r\n",
        "od = discriminator(ogr)\r\n",
        "gtd = torch.ones(256,1).cuda()\r\n",
        "loss = nn.BCELoss()\r\n",
        "lossout = loss(od,gtd)\r\n",
        "lossout.backward()\r\n",
        "D_optimizer.step()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiyijcV4twir"
      },
      "source": [
        "x = torch.ones(256,2,200).float().cuda()\r\n",
        "out = discriminator(x).shape"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6qamnAbUaHn",
        "outputId": "11e85cd9-afff-4f24-8c8f-153abc14d704",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.ones(1,2,1000).float().cuda()\r\n",
        "discriminator.setBatchSize(batch_size = 1)\r\n",
        "print(discriminator.init_hidden()[0].shape)\r\n",
        "out = discriminator(x)\r\n",
        "print(out)\r\n",
        "print(out.shape)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6, 1, 120])\n",
            "tensor([[0.5171]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "torch.Size([1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7I1a7olacs8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}